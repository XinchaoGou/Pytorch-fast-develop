{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning\n",
    "## 1. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Hyper Parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = np.array([[2.3], [4.4], [3.7], [6.1], [7.3], [2.1],[5.6], [7.7], [8.7], [4.1],\n",
    "\n",
    "                    [6.7], [6.1], [7.5], [2.1], [7.2],\n",
    "\n",
    "                    [5.6], [5.7], [7.7], [3.1]], dtype=np.float32)\n",
    "\n",
    "#xtrain生成矩阵数据\n",
    "\n",
    "y_train = np.array([[3.7], [4.76], [4.], [7.1], [8.6], [3.5],[5.4], [7.6], [7.9], [5.3],\n",
    "\n",
    "                    [7.3], [7.5], [8.5], [3.2], [8.7],\n",
    "\n",
    "                    [6.4], [6.6], [7.9], [5.3]], dtype=np.float32)\n",
    "plt.figure() \n",
    "#画图散点图\n",
    "plt.scatter(x_train,y_train)\n",
    "plt.xlabel('x_train')\n",
    "#x轴名称\n",
    "plt.ylabel('y_train')\n",
    "#y轴名称\n",
    "#显示图片\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LinearRegression(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000], Loss: 67.2495\n",
      "Epoch [10/1000], Loss: 32.9390\n",
      "Epoch [15/1000], Loss: 16.2488\n",
      "Epoch [20/1000], Loss: 8.1298\n",
      "Epoch [25/1000], Loss: 4.1802\n",
      "Epoch [30/1000], Loss: 2.2589\n",
      "Epoch [35/1000], Loss: 1.3242\n",
      "Epoch [40/1000], Loss: 0.8694\n",
      "Epoch [45/1000], Loss: 0.6481\n",
      "Epoch [50/1000], Loss: 0.5404\n",
      "Epoch [55/1000], Loss: 0.4879\n",
      "Epoch [60/1000], Loss: 0.4623\n",
      "Epoch [65/1000], Loss: 0.4498\n",
      "Epoch [70/1000], Loss: 0.4436\n",
      "Epoch [75/1000], Loss: 0.4405\n",
      "Epoch [80/1000], Loss: 0.4389\n",
      "Epoch [85/1000], Loss: 0.4381\n",
      "Epoch [90/1000], Loss: 0.4376\n",
      "Epoch [95/1000], Loss: 0.4373\n",
      "Epoch [100/1000], Loss: 0.4370\n",
      "Epoch [105/1000], Loss: 0.4368\n",
      "Epoch [110/1000], Loss: 0.4367\n",
      "Epoch [115/1000], Loss: 0.4365\n",
      "Epoch [120/1000], Loss: 0.4364\n",
      "Epoch [125/1000], Loss: 0.4362\n",
      "Epoch [130/1000], Loss: 0.4360\n",
      "Epoch [135/1000], Loss: 0.4359\n",
      "Epoch [140/1000], Loss: 0.4357\n",
      "Epoch [145/1000], Loss: 0.4356\n",
      "Epoch [150/1000], Loss: 0.4354\n",
      "Epoch [155/1000], Loss: 0.4353\n",
      "Epoch [160/1000], Loss: 0.4351\n",
      "Epoch [165/1000], Loss: 0.4350\n",
      "Epoch [170/1000], Loss: 0.4348\n",
      "Epoch [175/1000], Loss: 0.4347\n",
      "Epoch [180/1000], Loss: 0.4345\n",
      "Epoch [185/1000], Loss: 0.4344\n",
      "Epoch [190/1000], Loss: 0.4343\n",
      "Epoch [195/1000], Loss: 0.4341\n",
      "Epoch [200/1000], Loss: 0.4340\n",
      "Epoch [205/1000], Loss: 0.4338\n",
      "Epoch [210/1000], Loss: 0.4337\n",
      "Epoch [215/1000], Loss: 0.4335\n",
      "Epoch [220/1000], Loss: 0.4334\n",
      "Epoch [225/1000], Loss: 0.4332\n",
      "Epoch [230/1000], Loss: 0.4331\n",
      "Epoch [235/1000], Loss: 0.4329\n",
      "Epoch [240/1000], Loss: 0.4328\n",
      "Epoch [245/1000], Loss: 0.4327\n",
      "Epoch [250/1000], Loss: 0.4325\n",
      "Epoch [255/1000], Loss: 0.4324\n",
      "Epoch [260/1000], Loss: 0.4322\n",
      "Epoch [265/1000], Loss: 0.4321\n",
      "Epoch [270/1000], Loss: 0.4319\n",
      "Epoch [275/1000], Loss: 0.4318\n",
      "Epoch [280/1000], Loss: 0.4317\n",
      "Epoch [285/1000], Loss: 0.4315\n",
      "Epoch [290/1000], Loss: 0.4314\n",
      "Epoch [295/1000], Loss: 0.4312\n",
      "Epoch [300/1000], Loss: 0.4311\n",
      "Epoch [305/1000], Loss: 0.4310\n",
      "Epoch [310/1000], Loss: 0.4308\n",
      "Epoch [315/1000], Loss: 0.4307\n",
      "Epoch [320/1000], Loss: 0.4305\n",
      "Epoch [325/1000], Loss: 0.4304\n",
      "Epoch [330/1000], Loss: 0.4303\n",
      "Epoch [335/1000], Loss: 0.4301\n",
      "Epoch [340/1000], Loss: 0.4300\n",
      "Epoch [345/1000], Loss: 0.4298\n",
      "Epoch [350/1000], Loss: 0.4297\n",
      "Epoch [355/1000], Loss: 0.4296\n",
      "Epoch [360/1000], Loss: 0.4294\n",
      "Epoch [365/1000], Loss: 0.4293\n",
      "Epoch [370/1000], Loss: 0.4292\n",
      "Epoch [375/1000], Loss: 0.4290\n",
      "Epoch [380/1000], Loss: 0.4289\n",
      "Epoch [385/1000], Loss: 0.4288\n",
      "Epoch [390/1000], Loss: 0.4286\n",
      "Epoch [395/1000], Loss: 0.4285\n",
      "Epoch [400/1000], Loss: 0.4284\n",
      "Epoch [405/1000], Loss: 0.4282\n",
      "Epoch [410/1000], Loss: 0.4281\n",
      "Epoch [415/1000], Loss: 0.4280\n",
      "Epoch [420/1000], Loss: 0.4278\n",
      "Epoch [425/1000], Loss: 0.4277\n",
      "Epoch [430/1000], Loss: 0.4276\n",
      "Epoch [435/1000], Loss: 0.4274\n",
      "Epoch [440/1000], Loss: 0.4273\n",
      "Epoch [445/1000], Loss: 0.4272\n",
      "Epoch [450/1000], Loss: 0.4270\n",
      "Epoch [455/1000], Loss: 0.4269\n",
      "Epoch [460/1000], Loss: 0.4268\n",
      "Epoch [465/1000], Loss: 0.4267\n",
      "Epoch [470/1000], Loss: 0.4265\n",
      "Epoch [475/1000], Loss: 0.4264\n",
      "Epoch [480/1000], Loss: 0.4263\n",
      "Epoch [485/1000], Loss: 0.4261\n",
      "Epoch [490/1000], Loss: 0.4260\n",
      "Epoch [495/1000], Loss: 0.4259\n",
      "Epoch [500/1000], Loss: 0.4258\n",
      "Epoch [505/1000], Loss: 0.4256\n",
      "Epoch [510/1000], Loss: 0.4255\n",
      "Epoch [515/1000], Loss: 0.4254\n",
      "Epoch [520/1000], Loss: 0.4252\n",
      "Epoch [525/1000], Loss: 0.4251\n",
      "Epoch [530/1000], Loss: 0.4250\n",
      "Epoch [535/1000], Loss: 0.4249\n",
      "Epoch [540/1000], Loss: 0.4247\n",
      "Epoch [545/1000], Loss: 0.4246\n",
      "Epoch [550/1000], Loss: 0.4245\n",
      "Epoch [555/1000], Loss: 0.4244\n",
      "Epoch [560/1000], Loss: 0.4242\n",
      "Epoch [565/1000], Loss: 0.4241\n",
      "Epoch [570/1000], Loss: 0.4240\n",
      "Epoch [575/1000], Loss: 0.4239\n",
      "Epoch [580/1000], Loss: 0.4238\n",
      "Epoch [585/1000], Loss: 0.4236\n",
      "Epoch [590/1000], Loss: 0.4235\n",
      "Epoch [595/1000], Loss: 0.4234\n",
      "Epoch [600/1000], Loss: 0.4233\n",
      "Epoch [605/1000], Loss: 0.4231\n",
      "Epoch [610/1000], Loss: 0.4230\n",
      "Epoch [615/1000], Loss: 0.4229\n",
      "Epoch [620/1000], Loss: 0.4228\n",
      "Epoch [625/1000], Loss: 0.4227\n",
      "Epoch [630/1000], Loss: 0.4225\n",
      "Epoch [635/1000], Loss: 0.4224\n",
      "Epoch [640/1000], Loss: 0.4223\n",
      "Epoch [645/1000], Loss: 0.4222\n",
      "Epoch [650/1000], Loss: 0.4221\n",
      "Epoch [655/1000], Loss: 0.4219\n",
      "Epoch [660/1000], Loss: 0.4218\n",
      "Epoch [665/1000], Loss: 0.4217\n",
      "Epoch [670/1000], Loss: 0.4216\n",
      "Epoch [675/1000], Loss: 0.4215\n",
      "Epoch [680/1000], Loss: 0.4214\n",
      "Epoch [685/1000], Loss: 0.4212\n",
      "Epoch [690/1000], Loss: 0.4211\n",
      "Epoch [695/1000], Loss: 0.4210\n",
      "Epoch [700/1000], Loss: 0.4209\n",
      "Epoch [705/1000], Loss: 0.4208\n",
      "Epoch [710/1000], Loss: 0.4207\n",
      "Epoch [715/1000], Loss: 0.4205\n",
      "Epoch [720/1000], Loss: 0.4204\n",
      "Epoch [725/1000], Loss: 0.4203\n",
      "Epoch [730/1000], Loss: 0.4202\n",
      "Epoch [735/1000], Loss: 0.4201\n",
      "Epoch [740/1000], Loss: 0.4200\n",
      "Epoch [745/1000], Loss: 0.4199\n",
      "Epoch [750/1000], Loss: 0.4197\n",
      "Epoch [755/1000], Loss: 0.4196\n",
      "Epoch [760/1000], Loss: 0.4195\n",
      "Epoch [765/1000], Loss: 0.4194\n",
      "Epoch [770/1000], Loss: 0.4193\n",
      "Epoch [775/1000], Loss: 0.4192\n",
      "Epoch [780/1000], Loss: 0.4191\n",
      "Epoch [785/1000], Loss: 0.4190\n",
      "Epoch [790/1000], Loss: 0.4189\n",
      "Epoch [795/1000], Loss: 0.4187\n",
      "Epoch [800/1000], Loss: 0.4186\n",
      "Epoch [805/1000], Loss: 0.4185\n",
      "Epoch [810/1000], Loss: 0.4184\n",
      "Epoch [815/1000], Loss: 0.4183\n",
      "Epoch [820/1000], Loss: 0.4182\n",
      "Epoch [825/1000], Loss: 0.4181\n",
      "Epoch [830/1000], Loss: 0.4180\n",
      "Epoch [835/1000], Loss: 0.4179\n",
      "Epoch [840/1000], Loss: 0.4178\n",
      "Epoch [845/1000], Loss: 0.4176\n",
      "Epoch [850/1000], Loss: 0.4175\n",
      "Epoch [855/1000], Loss: 0.4174\n",
      "Epoch [860/1000], Loss: 0.4173\n",
      "Epoch [865/1000], Loss: 0.4172\n",
      "Epoch [870/1000], Loss: 0.4171\n",
      "Epoch [875/1000], Loss: 0.4170\n",
      "Epoch [880/1000], Loss: 0.4169\n",
      "Epoch [885/1000], Loss: 0.4168\n",
      "Epoch [890/1000], Loss: 0.4167\n",
      "Epoch [895/1000], Loss: 0.4166\n",
      "Epoch [900/1000], Loss: 0.4165\n",
      "Epoch [905/1000], Loss: 0.4164\n",
      "Epoch [910/1000], Loss: 0.4163\n",
      "Epoch [915/1000], Loss: 0.4162\n",
      "Epoch [920/1000], Loss: 0.4161\n",
      "Epoch [925/1000], Loss: 0.4159\n",
      "Epoch [930/1000], Loss: 0.4158\n",
      "Epoch [935/1000], Loss: 0.4157\n",
      "Epoch [940/1000], Loss: 0.4156\n",
      "Epoch [945/1000], Loss: 0.4155\n",
      "Epoch [950/1000], Loss: 0.4154\n",
      "Epoch [955/1000], Loss: 0.4153\n",
      "Epoch [960/1000], Loss: 0.4152\n",
      "Epoch [965/1000], Loss: 0.4151\n",
      "Epoch [970/1000], Loss: 0.4150\n",
      "Epoch [975/1000], Loss: 0.4149\n",
      "Epoch [980/1000], Loss: 0.4148\n",
      "Epoch [985/1000], Loss: 0.4147\n",
      "Epoch [990/1000], Loss: 0.4146\n",
      "Epoch [995/1000], Loss: 0.4145\n",
      "Epoch [1000/1000], Loss: 0.4144\n"
     ]
    }
   ],
   "source": [
    "# Train the Model \n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy array to torch Variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    targets = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Forward + Backward + Optimize\n",
    "    optimizer.zero_grad()  \n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3]\n",
      " [4.4]\n",
      " [3.7]\n",
      " [6.1]\n",
      " [7.3]\n",
      " [2.1]\n",
      " [5.6]\n",
      " [7.7]\n",
      " [8.7]\n",
      " [4.1]\n",
      " [6.7]\n",
      " [6.1]\n",
      " [7.5]\n",
      " [2.1]\n",
      " [7.2]\n",
      " [5.6]\n",
      " [5.7]\n",
      " [7.7]\n",
      " [3.1]]\n",
      "tensor([[3.2844],\n",
      "        [5.2278],\n",
      "        [4.5800],\n",
      "        [6.8011],\n",
      "        [7.9116],\n",
      "        [3.0993],\n",
      "        [6.3384],\n",
      "        [8.2818],\n",
      "        [9.2073],\n",
      "        [4.9502],\n",
      "        [7.3564],\n",
      "        [6.8011],\n",
      "        [8.0967],\n",
      "        [3.0993],\n",
      "        [7.8191],\n",
      "        [6.3384],\n",
      "        [6.4309],\n",
      "        [8.2818],\n",
      "        [4.0247]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Plot the graph\n",
    "\n",
    "model.eval()\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "plt.plot(x_train, y_train, 'ro')\n",
    "plt.plot(x_train, predicted, label='predict')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(x_train)\n",
    "print(model(Variable(torch.from_numpy(x_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST Dataset (Images and Labels)\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = LogisticRegression(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [100/1200], Loss: 2.1831\n",
      "Epoch: [1/10], Step: [200/1200], Loss: 2.1507\n",
      "Epoch: [1/10], Step: [300/1200], Loss: 2.0765\n",
      "Epoch: [1/10], Step: [400/1200], Loss: 1.9321\n",
      "Epoch: [1/10], Step: [500/1200], Loss: 1.9303\n",
      "Epoch: [1/10], Step: [600/1200], Loss: 1.8002\n",
      "Epoch: [1/10], Step: [700/1200], Loss: 1.6758\n",
      "Epoch: [1/10], Step: [800/1200], Loss: 1.7074\n",
      "Epoch: [1/10], Step: [900/1200], Loss: 1.6424\n",
      "Epoch: [1/10], Step: [1000/1200], Loss: 1.6328\n",
      "Epoch: [1/10], Step: [1100/1200], Loss: 1.5840\n",
      "Epoch: [1/10], Step: [1200/1200], Loss: 1.4793\n",
      "Epoch: [2/10], Step: [100/1200], Loss: 1.4027\n",
      "Epoch: [2/10], Step: [200/1200], Loss: 1.5436\n",
      "Epoch: [2/10], Step: [300/1200], Loss: 1.2832\n",
      "Epoch: [2/10], Step: [400/1200], Loss: 1.3679\n",
      "Epoch: [2/10], Step: [500/1200], Loss: 1.3373\n",
      "Epoch: [2/10], Step: [600/1200], Loss: 1.3346\n",
      "Epoch: [2/10], Step: [700/1200], Loss: 1.2112\n",
      "Epoch: [2/10], Step: [800/1200], Loss: 1.2671\n",
      "Epoch: [2/10], Step: [900/1200], Loss: 1.1575\n",
      "Epoch: [2/10], Step: [1000/1200], Loss: 1.0817\n",
      "Epoch: [2/10], Step: [1100/1200], Loss: 1.1489\n",
      "Epoch: [2/10], Step: [1200/1200], Loss: 1.0504\n",
      "Epoch: [3/10], Step: [100/1200], Loss: 1.1200\n",
      "Epoch: [3/10], Step: [200/1200], Loss: 1.1736\n",
      "Epoch: [3/10], Step: [300/1200], Loss: 1.0018\n",
      "Epoch: [3/10], Step: [400/1200], Loss: 1.0307\n",
      "Epoch: [3/10], Step: [500/1200], Loss: 0.9958\n",
      "Epoch: [3/10], Step: [600/1200], Loss: 0.9419\n",
      "Epoch: [3/10], Step: [700/1200], Loss: 1.1025\n",
      "Epoch: [3/10], Step: [800/1200], Loss: 1.0402\n",
      "Epoch: [3/10], Step: [900/1200], Loss: 0.9009\n",
      "Epoch: [3/10], Step: [1000/1200], Loss: 0.8081\n",
      "Epoch: [3/10], Step: [1100/1200], Loss: 0.9216\n",
      "Epoch: [3/10], Step: [1200/1200], Loss: 0.9920\n",
      "Epoch: [4/10], Step: [100/1200], Loss: 0.9250\n",
      "Epoch: [4/10], Step: [200/1200], Loss: 0.9877\n",
      "Epoch: [4/10], Step: [300/1200], Loss: 0.8650\n",
      "Epoch: [4/10], Step: [400/1200], Loss: 0.9803\n",
      "Epoch: [4/10], Step: [500/1200], Loss: 0.8830\n",
      "Epoch: [4/10], Step: [600/1200], Loss: 0.7872\n",
      "Epoch: [4/10], Step: [700/1200], Loss: 0.6756\n",
      "Epoch: [4/10], Step: [800/1200], Loss: 0.9900\n",
      "Epoch: [4/10], Step: [900/1200], Loss: 0.8422\n",
      "Epoch: [4/10], Step: [1000/1200], Loss: 0.7964\n",
      "Epoch: [4/10], Step: [1100/1200], Loss: 0.7227\n",
      "Epoch: [4/10], Step: [1200/1200], Loss: 0.9336\n",
      "Epoch: [5/10], Step: [100/1200], Loss: 0.7459\n",
      "Epoch: [5/10], Step: [200/1200], Loss: 0.7080\n",
      "Epoch: [5/10], Step: [300/1200], Loss: 0.7891\n",
      "Epoch: [5/10], Step: [400/1200], Loss: 0.8319\n",
      "Epoch: [5/10], Step: [500/1200], Loss: 0.9318\n",
      "Epoch: [5/10], Step: [600/1200], Loss: 0.7540\n",
      "Epoch: [5/10], Step: [700/1200], Loss: 0.6991\n",
      "Epoch: [5/10], Step: [800/1200], Loss: 0.7532\n",
      "Epoch: [5/10], Step: [900/1200], Loss: 0.8086\n",
      "Epoch: [5/10], Step: [1000/1200], Loss: 0.6687\n",
      "Epoch: [5/10], Step: [1100/1200], Loss: 0.7218\n",
      "Epoch: [5/10], Step: [1200/1200], Loss: 0.7587\n",
      "Epoch: [6/10], Step: [100/1200], Loss: 0.8942\n",
      "Epoch: [6/10], Step: [200/1200], Loss: 0.5874\n",
      "Epoch: [6/10], Step: [300/1200], Loss: 0.5362\n",
      "Epoch: [6/10], Step: [400/1200], Loss: 0.7628\n",
      "Epoch: [6/10], Step: [500/1200], Loss: 0.7419\n",
      "Epoch: [6/10], Step: [600/1200], Loss: 0.7283\n",
      "Epoch: [6/10], Step: [700/1200], Loss: 0.8380\n",
      "Epoch: [6/10], Step: [800/1200], Loss: 0.7954\n",
      "Epoch: [6/10], Step: [900/1200], Loss: 0.6340\n",
      "Epoch: [6/10], Step: [1000/1200], Loss: 0.7281\n",
      "Epoch: [6/10], Step: [1100/1200], Loss: 0.6411\n",
      "Epoch: [6/10], Step: [1200/1200], Loss: 0.6964\n",
      "Epoch: [7/10], Step: [100/1200], Loss: 0.6603\n",
      "Epoch: [7/10], Step: [200/1200], Loss: 0.7336\n",
      "Epoch: [7/10], Step: [300/1200], Loss: 0.8018\n",
      "Epoch: [7/10], Step: [400/1200], Loss: 0.8150\n",
      "Epoch: [7/10], Step: [500/1200], Loss: 0.6397\n",
      "Epoch: [7/10], Step: [600/1200], Loss: 0.5669\n",
      "Epoch: [7/10], Step: [700/1200], Loss: 0.7864\n",
      "Epoch: [7/10], Step: [800/1200], Loss: 0.5252\n",
      "Epoch: [7/10], Step: [900/1200], Loss: 0.5781\n",
      "Epoch: [7/10], Step: [1000/1200], Loss: 0.6064\n",
      "Epoch: [7/10], Step: [1100/1200], Loss: 0.7721\n",
      "Epoch: [7/10], Step: [1200/1200], Loss: 0.6906\n",
      "Epoch: [8/10], Step: [100/1200], Loss: 0.5680\n",
      "Epoch: [8/10], Step: [200/1200], Loss: 0.6676\n",
      "Epoch: [8/10], Step: [300/1200], Loss: 0.6213\n",
      "Epoch: [8/10], Step: [400/1200], Loss: 0.5748\n",
      "Epoch: [8/10], Step: [500/1200], Loss: 0.5463\n",
      "Epoch: [8/10], Step: [600/1200], Loss: 0.5062\n",
      "Epoch: [8/10], Step: [700/1200], Loss: 0.6222\n",
      "Epoch: [8/10], Step: [800/1200], Loss: 0.5368\n",
      "Epoch: [8/10], Step: [900/1200], Loss: 0.5493\n",
      "Epoch: [8/10], Step: [1000/1200], Loss: 0.5394\n",
      "Epoch: [8/10], Step: [1100/1200], Loss: 0.6369\n",
      "Epoch: [8/10], Step: [1200/1200], Loss: 0.7125\n",
      "Epoch: [9/10], Step: [100/1200], Loss: 0.8459\n",
      "Epoch: [9/10], Step: [200/1200], Loss: 0.6545\n",
      "Epoch: [9/10], Step: [300/1200], Loss: 0.6803\n",
      "Epoch: [9/10], Step: [400/1200], Loss: 0.5486\n",
      "Epoch: [9/10], Step: [500/1200], Loss: 0.8261\n",
      "Epoch: [9/10], Step: [600/1200], Loss: 0.7417\n",
      "Epoch: [9/10], Step: [700/1200], Loss: 0.6449\n",
      "Epoch: [9/10], Step: [800/1200], Loss: 0.6320\n",
      "Epoch: [9/10], Step: [900/1200], Loss: 0.5207\n",
      "Epoch: [9/10], Step: [1000/1200], Loss: 0.5191\n",
      "Epoch: [9/10], Step: [1100/1200], Loss: 0.6170\n",
      "Epoch: [9/10], Step: [1200/1200], Loss: 0.6334\n",
      "Epoch: [10/10], Step: [100/1200], Loss: 0.4688\n",
      "Epoch: [10/10], Step: [200/1200], Loss: 0.7480\n",
      "Epoch: [10/10], Step: [300/1200], Loss: 0.6374\n",
      "Epoch: [10/10], Step: [400/1200], Loss: 0.5885\n",
      "Epoch: [10/10], Step: [500/1200], Loss: 0.6904\n",
      "Epoch: [10/10], Step: [600/1200], Loss: 0.6392\n",
      "Epoch: [10/10], Step: [700/1200], Loss: 0.5681\n",
      "Epoch: [10/10], Step: [800/1200], Loss: 0.5432\n",
      "Epoch: [10/10], Step: [900/1200], Loss: 0.5655\n",
      "Epoch: [10/10], Step: [1000/1200], Loss: 0.5410\n",
      "Epoch: [10/10], Step: [1100/1200], Loss: 0.8810\n",
      "Epoch: [10/10], Step: [1200/1200], Loss: 0.3876\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "                   % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 87 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Model\n",
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
