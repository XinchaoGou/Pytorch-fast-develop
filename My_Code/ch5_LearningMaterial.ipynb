{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.数据集\n",
    "## 1.1 Torchvision 包括的数据集\n",
    "* MNIST\n",
    "* COCO\n",
    "* LSUN Classification\n",
    "* ImageFolder\n",
    "* Imagenet-12\n",
    "* CIFAR10 and CIFAR100\n",
    "* STL10\n",
    "* SVHN\n",
    "* Photo Tour\n",
    "\n",
    "## 1.2 MNIST 介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.MNIST(root, train = True, transform = None, target_transform = None, download = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.data.DataLoader(dataset, batch_size = 1, shuffle = False, sampler = None, num_workers = 0, collate_fn = <function default_collate>, pin_memory = False, drop_last = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 torchvision.models 模块包含预训练模型结构\n",
    "* AlexNet\n",
    "* VGG\n",
    "* ResNet\n",
    "* SqueezeNet\n",
    "* DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用构造函数构造随机权重的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torchvision.models as models\n",
    "    resnet18 = models.resnet18()\n",
    "    alexnet = models.alexnet()\n",
    "    squeezenet = models.squeezenet1_0()\n",
    "#     densenet = models.densenet_161()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torchvision.models as models\n",
    "    resnet18 = models.resnet18(pretrained = True)\n",
    "    torchvisoin.models.alexnet(pretrained = False, ** kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 定义神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autograd \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "       x = F.relu(self.conv1(x))\n",
    "       return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 卷积层\n",
    "## 2.1 一维卷积\n",
    "    class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation = 1, groups = 1, bias = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(16, 33, 3, stride=2)\n",
    "inputs = autograd.Variable(torch.randn(20, 16, 50))\n",
    "output = m(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 池化层\n",
    "    class torch.nn.MaxPool1d(kernel_size, stride = None, padding =  0, dilation = 1, return_indices = False, ceil_mode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.MaxPool1d(3, stride=2)\n",
    "input = autograd.Variable(torch.randn(20, 16, 50))\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 小型ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MNISTConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, input):\n",
    "        x = self.pool1(F.relu(self.conv1(input)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        return x\n",
    "net = MNISTConvNet()\n",
    "print(net)\n",
    "input = Variable(torch.randn(1, 1, 28, 28))\n",
    "out = net(input)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Functional 函数\n",
    "## 3.1 torch.nn.functional 包里提供的函数\n",
    "* convolution 函数\n",
    "* Pooling\n",
    "* 非线性激活函数\n",
    "* Normalization 函数\n",
    "* 线性函数\n",
    "* Dropout 函数\n",
    "* 距离函数( Distance Functions )\n",
    "* 损失函数( Loss Function )\n",
    "* vision functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 torch.nn.functional.conv1d\n",
    "    torch.nn.functional.conv1d(input, weight, bias = None, stride = 1, padding = 0, dilation = 1, groups = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8862, -1.5707,  0.4922]]])\n",
      "tensor([[[ 1.3507,  0.7947,  0.8037]],\n",
      "\n",
      "        [[ 0.9339,  0.9322, -1.1509]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0495]],\n",
       "\n",
       "        [[-2.8583]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import autograd as autograd\n",
    "filters = autograd.Variable(torch.randn(1, 1, 3))\n",
    "inputs = autograd.Variable(torch.randn(2, 1, 3))\n",
    "print(filters)\n",
    "print(inputs)\n",
    "F.conv1d(inputs, filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pooling\n",
    "### 3.3.1 常见 Pooling\n",
    "* Mean-Pooling\n",
    "* Max-Pooling\n",
    "* Stochastic-Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 torch.nn.functional.avg_pool1d\n",
    "    torch.nn.functional.avg_pool1d (input, kernel_size, stride = None, padding = 0, ceil_mode = False, count_include_pad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 激活函数\n",
    "### 3.4.1 sigmoid 函数\n",
    "    torch.nn.functional.sigmoid(input)\n",
    "    \n",
    "$$ f(x)= \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "    m = nn.sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Tanh 函数\n",
    "    torch.nn.functional.tanh(input)\n",
    "$$ tanh(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}} $$\n",
    "\n",
    "    m = nn.tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 ReLU 函数\n",
    "    torch.nn.functional.relu(input, inplace = False)\n",
    "\n",
    "$$ y= \\begin{cases} 0, & x \\leq 0 \\\\ x, & x > 0 \\end{cases} $$\n",
    "\n",
    "    m = nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Dropout 函数\n",
    "### 常见的Dropout 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.dropout(input, p = 0.5, training = False, inplace = False)\n",
    "torch.nn.functional.alpha_dropout(input, p = 0.5, training = False)\n",
    "torch.nn.functional.dropout2d(input, p = 0.5, trainng = False, inplace = False)\n",
    "torch.nn.functional.dropout3d(input, p = 0.5, trainging = False, inplace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 损失函数（Loss functions）\n",
    "* 经验风险损失函数\n",
    "* 结构风险损失函数\n",
    "### 常见损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.kl_div(input, target, size_average=True)\n",
    "torch.nn.functional.poisson_nll_loss(input, target, log_input=Ture, full=False, size_average=True)\n",
    "torch.nn.functional.nll_loss(input, target, weight=None, size_average=True)\n",
    "torch.nn.functional.cosine_embedding_loss(input, input2, target, margin = 0, size_average=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
    "optimizer = optim.SGD(params, lr, momentum = 0, dampending = 0, weight_decay = 0, nesterov = False)\n",
    "optimizer = optim.Adam([var1, var2], lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1 指定每一层学习速率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.SGD([\n",
    "          {'params': model.base.parameters()},\n",
    "          {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "          ],lr = 1e-2, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 更新参数\n",
    "* optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input, target in dataset:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 一些优化算法如 Conjugate Gradient 和 LBFGS 需要重复多次计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input, target in dataset:\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 自动求导机制\n",
    "### 3.8.1 requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable as Variable\n",
    "x = Variable(torch.randn(5,5))\n",
    "y = Variable(torch.randn(5,5))\n",
    "z = Variable(torch.randn(5,5), requires_grad= True)\n",
    "a = x+y\n",
    "print(a.requires_grad)\n",
    "b = a+z\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 冻结训练好的模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained = True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(512,100)\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr = 1e-2, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.2 volatile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_input = Variable(torch.randn(5,5))\n",
    "volatile_input = Variable(torch.randn(5,5), volatile = True)\n",
    "model = torchvision.models.resnet18(pretrained = True)\n",
    "model(regular_input).requires_grad\n",
    "# True\n",
    "model(volatile_input).requirs_grad\n",
    "# False\n",
    "model(volatile_input).volatile\n",
    "# True\n",
    "model(volatile_input).creator is None\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 保存和加载模型\n",
    "* 只保存和加载模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(the_model.state_dict(),PATH)\n",
    "the_model = TheModelClass(* args, ** kwargs)\n",
    "the_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 保存和加载整个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(the_model, PATH)\n",
    "the_model = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 GPU加速运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Torch.cuda.current_device()\n",
    "torch.cuda.device(idx)\n",
    "x = torch.cuda.FloatTensor(1)\n",
    "y = torch.FloatTensor(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
